{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "L'objectif de ce notebook est d'illustrer le principe des GAN (*Generative Adversarial Networks*). Le GAN est un échange entre deux réseaux :    \n",
        "* Le générateur : son objectif est à partir de données aléatoires de générer des images qui sont le plus similaires possibles d'un jeu de données\n",
        "* Le discriminteur : son objectif est de parer les fausses images générées et d'éviter que celles-ci soient considérées comme vraies.     \n",
        "\n",
        "Les deux réseaux vont donc chacun faire en sorte de gagner : le générateur va essayer de générer des images les plus vraisemblables et le discriminateur va essayer de voir si les images générées sont vraies ou fausses.   \n",
        "\n",
        "Le principe est résumé sur la figure suivante :      \n",
        "\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/ArchitectureGAN\n",
        "\" align=\"center\" >\n",
        "\n",
        "A partir de données aléatoires, i.e. Random Noise, le générateur va générer des données. Ces dernières sont combinées à des données réelles et sont envoyées dans le discriminateur. L'objectif du discriminateur est alors de déterminer s'il s'agit de données réelles ou de fausses données. Il fait une classification binaire : données réelles ou données fausses. Bien entendu le générateur fait en sorte que les données générées aient le même label que les données réelles.    \n",
        "\n",
        "De manière à générer des données assez similaires aux réelles, le principe est le suivant : l'erreur à la fin du discriminateur est reportée au niveau du générateur afin qu'il mette ses poids à jour ... donc au fur et à mesure des epochs, la loss est projetée vers le générateur. Par contre il faut faire attention à ne pas projeter l'erreur du générateur à l'issue du modèle GAN au niveau du discriminateur ! toutes ces étapes sont illustrées par la suite.\n"
      ],
      "metadata": {
        "id": "W4bhs0ec9nNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "GyJfp8-VJtoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant de commencer, il est nécessaire de déjà posséder dans son environnement toutes les librairies utiles. Dans la seconde cellule nous importons toutes les librairies qui seront utiles à ce notebook. Il se peut que, lorsque vous lanciez l'éxecution de cette cellule, une soit absente. Dans ce cas il est nécessaire de l'installer. Pour cela dans la cellule suivante utiliser la commande :\n",
        "! pip install nom_librairie\n",
        "Attention : il est fortement conseillé lorsque l'une des librairies doit être installer de relancer le kernel de votre notebook.\n",
        "Remarque : même si toutes les librairies sont importées dès le début, les librairies utiles pour des fonctions présentées au cours de ce notebook sont ré-importées de manière à indiquer d'où elles viennent et ainsi faciliter la réutilisation de la fonction dans un autre projet."
      ],
      "metadata": {
        "id": "4H91x_RvJuUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utiliser cette cellule pour installer les librairies manquantes\n",
        "# pour cela il suffit de taper dans cette cellule : !pip install nom_librairie_manquante\n",
        "# d'exécuter la cellule et de relancer la cellule suivante pour voir si tout se passe bien\n",
        "# recommencer tant que toutes les librairies ne sont pas installées ..."
      ],
      "metadata": {
        "id": "GO7owGQDJu7c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n"
      ],
      "metadata": {
        "id": "WWBr8C3wLlll"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pour monter son drive Google Drive local\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WK-phCWFLmI1",
        "outputId": "3c3d0377-93a9-46e6-d92b-2da715950aad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "my_local_drive='/content/gdrive/My Drive/Colab Notebooks/ML_FDS'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "2ibLMr23L2r2",
        "outputId": "a705c12c-3cf8-4bd2-bdac-071d0350ed76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/ML_FDS\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/ML_FDS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tout d'abord nous allons créer un générateur. Celui-ci va prendre en entrée un vecteur de données générées aléatoirement de taille latent_dim. Généralement latent_dim est mis à 100 mais d'autres valeurs sont bien sûr possibles. Il suffit après de mettre au bon format. Pour le générer nous utiliserons par exemple  :   \n",
        "\n",
        "    np.random.randn(latent_dim * nombre d'exemples) ou np.random.normal(0,1, latent_dim)   \n",
        "\n",
        "Il va donc falloir traduire ce vecteur en tableau 2D de 28x28 (784 valeurs) à la sortie du générateur, i.e. il s'agit d'une image qui sera passé au discriminateur. Pour simplifier nous considérons que la couche d'entrée est une couche dense (il est possible de rentrer directement sur une convolution en transformant d'abord le vecteur d'entrée). Pour déterminer le nombre de neurones de la couche dense L'entrée du générateur étant un vecteur, nous allons utiliser une couche dense. Il existe plusieurs moyens de déterminer le nombre de neurones de cette couche d'entrée.  \n",
        "\n",
        "Ici nous allons utiliser du sur-échantillonage (*Conv2DTranspose*): l'idée est de ne générer qu'une partie de l'image et après de la dupliquer. Pour cela nous savons que la moitié d'une image fait 14×14 (soit 196 neurones), pour un quart de l'image nous avons donc 7x7 (49 neurones). Nous avons vu que les convolutions vont rechercher des motifs dans des images. Avec 49 neurones nous ne permettrons pas de faire de la convolution alors nous allons multiplier par un nombre qui permettent au réseau de faire des convolutions (en gros nous permettons d'avoir plusieurs images). Par exemple ici nous prenons 128 mais nous aurions pu mettre 256.   \n",
        "\n",
        "Notre réseau au début va ressembler à  :   \n",
        "\n",
        "    model = Sequential()\n",
        "    # On considère 1/4 de l'image (7x7)\n",
        "    # On multiplie par 128\n",
        "    # le nombre de neurones est donc \n",
        "\t  n_nodes = 128 * 7 * 7\n",
        "\t  model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    # Nous avons besoin d'une fonction d'activation\n",
        "    # ici nous prenons LeakyRely (à la place de 0 comme dans \n",
        "    # Relu, la valeur retournée sera alpha) qui évite des problèmes de mort de neurones\n",
        "\t  model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    \n",
        "Par la suite nous allons faire passer cette couche vers une convolution, pour cela il est nécessaire de changer le shape de la couche de sortie :  \n",
        "\n",
        "    model.add(reshape((7,7,128))  \n",
        "\n",
        "Il ne reste plus qu'à agrandir l'image (sur-échantillonage) avec la fonction Conv2DTranspose. Elle ressemble à une convolution normale et le stride (2x2) permet de quadrupler l'image (double la hauteur et la largeur)  :      \n",
        "\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "    # ici la sortie de l'image sera de 14x14 il faut le refaire pour avoir une image de taille 28x28\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "\n",
        "Il suffit à la fin de mettre une couche de sortie avec une sigmoid ou une autre fonction d'activation comme tanh (pour avoir des valeurs comprises entre 0 et 1). Ici nous avons un seul feature map en sortie Attention ne pas mettre un softmax car lors de la rétro-propagation du gradient le softmax ne se dérive pas simplement (c'est pourquoi la génération de textes est plus compliquée avec des GAN !).    \n",
        "\n",
        "    model.add(Conv2D(1, (7,7), activation='sigmoid',   padding='same'))   \n",
        "\n",
        "Remarque nous aurions pu finir au second Conv2Dtranspose qui possède une sortie de la bonne taille : 28x28 et simplement mettre une fonction d'activation tanh dans la convolution.\n",
        "\n"
      ],
      "metadata": {
        "id": "gA_6j26z-Lrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des données\n",
        "def load_data():\n",
        "  fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "  (X_trainX, _), (_, _) = fashion_mnist.load_data()\n",
        "  # Ajout d'une dimension pour le canal (1)\n",
        "  X = expand_dims(trainX, axis=-1)\n",
        "  # Normalisation\n",
        "  X = X.astype('float32')\n",
        "  # scale from [0,255] to [0,1]\n",
        "  X = X / 255.0\n",
        "  return X"
      ],
      "metadata": {
        "id": "KJmg7uj8G7C0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction ci-dessous reprend le générateur. Nous avons ajouté des BatchNormalization pour faciliter l'apprentissage :"
      ],
      "metadata": {
        "id": "00NRlbjQ1xv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_generator(latent_dim=100):\n",
        "  model = Sequential()\n",
        "\t# 1/4 de l'image (7x7) * nombre d'images possibles\n",
        "  n_nodes = 128 * 7 * 7\n",
        "  model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((7, 7, 128)))\n",
        "  # upsample to 14x14\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(BatchNormalization()) \t\t\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  # upsample to 28x28\n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "hxbNmxdl97xw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La définition de la partie du générateur est :        "
      ],
      "metadata": {
        "id": "0Gvzy3-6rde-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator_model=create_generator()\n",
        "generator_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tM3vrT40rmC5",
        "outputId": "bb15e1ba-d831-43b0-d1c8-d1fbdf2508f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 6272)              633472    \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 6272)             25088     \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 6272)              0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 14, 14, 128)      262272    \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 14, 14, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 128)      262272    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 28, 28, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 28, 28, 128)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 1)         6273      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,190,401\n",
            "Trainable params: 1,177,345\n",
            "Non-trainable params: 13,056\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons tester le générateur pour voir un exemple d'image générée :     "
      ],
      "metadata": {
        "id": "K3LjJHqmaRtL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim=100\n",
        "generator = create_generator()\n",
        "noise=tf.random.normal([1,latent_dim])\n",
        "generated_image = generator (noise, training=False)\n",
        "plt.imshow (generated_image[0, :, :, 0], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "-T_WPQe5YjNB",
        "outputId": "bb77c1f5-a2e4-48fb-878a-b5c0e1756455"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6c0e23b5d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le discriminateur est un CNN assez classique dans lequel en entrée il a des images de taille 28x28 et en sortie il possède un neurone avec comme fonction d'activation une sigmoid afin de déterminer s'il s'agit d'une image réelle ou générée. "
      ],
      "metadata": {
        "id": "E6BjQLREacDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_discriminator():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=[28,28,1]))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "yCWEZBu7bB9J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle pour le discriminateur est :       "
      ],
      "metadata": {
        "id": "i3JMnDQjrCVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_discriminator=create_discriminator()\n",
        "model_discriminator.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iM4Zmbm7rNUt",
        "outputId": "b2a92eeb-523c-44b2-9ee4-7c85def00a04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 14, 14, 64)        640       \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 7, 7, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 3137      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,705\n",
            "Trainable params: 40,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Même s'il n'a pas encore appris on peut le tester sur l'image générée précédente :    "
      ],
      "metadata": {
        "id": "I8IDpGSnbnGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator=create_discriminator()\n",
        "prediction=discriminator(generated_image)\n",
        "print (prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BLltp5PXhFNF",
        "outputId": "a2a8b3eb-d612-4a42-d671-af8b96781fbb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.47208416]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons le tester sur les images de Fashion Mnist pour voir si le modèle est capable de bien apprendre. Pour cela il faut définir quelques fonctions utiles :     "
      ],
      "metadata": {
        "id": "pWW2EXchizAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement et normalisation des données\n",
        "def load_data():\n",
        "  fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "  (X_train, _), (_, _) = fashion_mnist.load_data()\n",
        "\t\n",
        "  # Ajout d'une dimension pour le canal (1)\n",
        "  X = expand_dims(X_train, axis=-1)\n",
        "  # Normalisation\n",
        "  X = X.astype('float32')\n",
        "  X = X / 255.0\n",
        "  return X\n",
        "\n",
        "# Creation d'un jeu de données de vraies images\n",
        "# les vraies images sont labélisées avec 1\n",
        "def generate_real_samples(dataset, nb_images):\n",
        "\t# tirage aléatoire\n",
        "\tix = randint(0, dataset.shape[0], nb_images)\n",
        "\t# sélection des images\n",
        "\tX = dataset[ix]\n",
        "\t# mettre 1 comme label de classe\n",
        "\ty = ones((nb_images, 1))\n",
        "\treturn X, y\n",
        "\n",
        "# Création d'un faux jeu de données \n",
        "# elles seront labélisées avec 0\n",
        "def generate_fake_samples(nb_images):\n",
        "  # generation de nombres aléatoires entre 0 et 1\n",
        "  # attention ici on simplifie en mettant la taille de l'image\n",
        "  # normalement il faut prendre latent_dim\n",
        "  X = np.random.rand(28 * 28 * nb_images)\n",
        "  # reshape en images grises\n",
        "  X = X.reshape((nb_images, 28, 28, 1))\n",
        "  # mettre 0 comme label de classe\n",
        "  y = zeros((nb_images, 1))\n",
        "  return X, y  "
      ],
      "metadata": {
        "id": "iEcxTuMnbtpj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contrairement à ce que nous faisons d'habitude nous n'allons pas faire un fit mais construire l'entraînement du modèle. En d'autres termes nous ré-écrivons la fonction *fit*. Pour cela il faut boucler sur le nombre d'epochs (en fait ici il ne s'agit pas vraiment d'epoch car nous ne lançons qu'un batch par passage dans la boucle et non pas un passage sur l'ensemble du jeu de données) et lancer l'entraînement du modèle par batch. Ici nous envoyons au modèle un batch d'images réelles puis un batch de fausses images (elles sont de la même qualité que l'image précédente) car il n'y a aucun report de l'erreur sur le générateur. Cela nous permet d'évaluer l'accuracy du modèle pour les vraies et fausses images.   \n",
        "\n",
        "Nous utilisons la méthode *train_on_batch*. L'avantage de cette méthode est qu'elle va mettre à jour l'ensemble des poids en fonction des données que nous lui fournissons. Elle est par exemple utilisée lorsque vous avez déjà un modèle appris et que vous récupérez des données que vous n'avez jamais vu : elle va adapter les poids de votre modèle entraîné pour prendre en considération ces nouvelles données.  *fit* permet de considérer tout le jeu de données et parfois il est bien utile de pouvoir mettre à jour un modèle uniquement sur des nouvelles données et ne pas relancer l'apprentissage complet en intégrant les nouvelles données. "
      ],
      "metadata": {
        "id": "Ogti3Y2nfG9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the discriminator model\n",
        "def train_discriminator(model, dataset, epochs=100, batchsize=256):\n",
        "  # on constitue un jeu de données de batchsize/2 images réelles et images fausses\n",
        "\thalf_batch = int(batchsize / 2)\n",
        "\t# boucler sur les epochs\n",
        "\tfor i in range(epochs):\n",
        "\t\t# sélection d'images réelles\n",
        "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t# mettre à jour le discriminateur avec les images réelles\n",
        "\t\t_, real_acc = model.train_on_batch(X_real, y_real)\n",
        "\t\t# generation de fausses images \n",
        "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
        "\t\t# mise à jour du discriminateur avec de fausses images \n",
        "\t\t_, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "\t\t# Affichage des résultats pour l'accuracy des vraies et des fausses\n",
        "\t\tprint('>%d accuracy_real=%.0f%% accuracy_fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
      ],
      "metadata": {
        "id": "Wm_ITGORfHqP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il suffit à présent de lancer le modèle :    \n",
        "\n",
        "**Remarque** ici on met un nombre d'epochs faible (**pensons CO2**) c'est juste pour voir si l'accuracy augmente bien. Si vous mettez un nombre d'epochs à 100 vous verrez que le modèle arrive à 100% pour les deux : il est capable de faire la distinction entre les vraies images et les fausses."
      ],
      "metadata": {
        "id": "pJNNb4pkf6R_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_images_total=256 # 128 images de chaque classe\n",
        "epochs=10\n",
        "\n",
        "# chargement du modèle\n",
        "model = create_discriminator()\n",
        "\n",
        "\n",
        "# chargement des données\n",
        "dataset=load_data()\n",
        "\n",
        "# lancement de l'entrainement du discriminateur\n",
        "train_discriminator(model, dataset, epochs, nb_images_total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zXimkMRSf62K",
        "outputId": "ffbd44bd-fd21-467e-cc8e-9bb579e5e6c0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            ">1 accuracy_real=97% accuracy_fake=0%\n",
            ">2 accuracy_real=95% accuracy_fake=1%\n",
            ">3 accuracy_real=93% accuracy_fake=3%\n",
            ">4 accuracy_real=95% accuracy_fake=5%\n",
            ">5 accuracy_real=98% accuracy_fake=12%\n",
            ">6 accuracy_real=95% accuracy_fake=18%\n",
            ">7 accuracy_real=91% accuracy_fake=40%\n",
            ">8 accuracy_real=89% accuracy_fake=50%\n",
            ">9 accuracy_real=90% accuracy_fake=72%\n",
            ">10 accuracy_real=85% accuracy_fake=91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que nous avons testé le générateur et le discriminateur, nous pouvons construire le GAN. Celui-ci va prendre une partie générateur et une partie discriminateur. Attention il est important de reporter l'erreur de la sortie du discriminateur vers le générateur pour que celui-ci puisse modifier ses poids et améliorer l'image. De la même manière l'objectif ici n'est pas de modifier les poids du discriminateur mais plutôt d'envoyer vers l'arrière, i.e. le générateur, la descente de gradient. Pour cela on empêche le modèle d'apprendre le poids de l'erreur du GAN sur le discriminateur.   \n",
        "\n",
        "**Remarque** : si le discriminateur ne met pas à jour ses poids comment peut il s'améliorer alors qu'au départ les poids du discriminateur sont aléatoires ? **ce qu'il faut comprendre** la réponse à cette question est tout simplement dans l'utilisation de la méthode *train_on_batch*. Ce que nous voulons ici c'est que ce soit le generateur qui mette à jour ses poids. Par contre comme nous avons vu dans la fonction *train_discriminateur* en passant par *train_on_batch* les poids du discriminateur seront automatiquement mis à jour mais pas les erreurs remontées par le GAN.\n"
      ],
      "metadata": {
        "id": "AYcoaMpflNsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gan(generator_model, discriminator_model):\n",
        "  # mettre les poids du discriminateur non entrable\n",
        "  discriminator_model.trainable = False\n",
        "  # un seul modele qui regroupe \n",
        "  model = Sequential()\n",
        "  # ajout du générateur\n",
        "  model.add(generator_model)\n",
        "  # ajout du discriminateur\n",
        "  model.add(discriminator_model)\n",
        "\n",
        "  opt = Adam(learning_rate=0.0003, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  return model"
      ],
      "metadata": {
        "id": "FhdlTkisl9wY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle GAN final ressemble à ceci :"
      ],
      "metadata": {
        "id": "gsvgFVWstDHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "\n",
        "# creation du générateur\n",
        "generator_model = create_generator(latent_dim)\n",
        "# creation du discriminateur\n",
        "discriminator_model = create_discriminator()\n",
        "\n",
        "# creation du gan\n",
        "gan_model = create_gan(generator_model, discriminator_model)\n",
        "\n",
        "gan_model.summary()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "w8nqyBkUtEDN",
        "outputId": "58467e9b-f20b-4caf-9294-840fa336e388"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_5 (Sequential)   (None, 28, 28, 1)         1190401   \n",
            "                                                                 \n",
            " sequential_6 (Sequential)   (None, 1)                 40705     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,231,106\n",
            "Trainable params: 1,177,345\n",
            "Non-trainable params: 53,761\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant de finaliser l'entraînement du générateur et du discriminateur puis du GAN, nous définissons quelques fonctions utiles dans un contexte plus général que précédemment. Une fonction *generate_latent_points*pour générer dees données (i.e. pour générer un vecteur) aléatoires, une fonction pour génerer de fausses images, *generate_fake_samples_for_generator* dont l'objectif est à partir de points latent de prédire le résultat, i.e la sortie du modèle pour cela on prédit la sortie associée à une entrée aléatoire, une fonction *save_plot* qui affiche à différentes epochs les images générées, une fonction *evaluate_model* qui simule l'évaluation d'un jeu de test (i.e. le jeu de données validation dans un fit). Cette dernière n'est pas appelée à toutes les étapes pour gagner du temps.  "
      ],
      "metadata": {
        "id": "EOmz4xHPs3RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_latent_points (latent_dim, nb_images):\n",
        "  X_input = np.random.randn(latent_dim * nb_images)\n",
        "  X_input = X_input.reshape(nb_images, latent_dim)\n",
        "  return X_input\n",
        "\n",
        "def generate_fake_samples_for_generator(generator_model, latent_dim, nb_images):\n",
        "  # generation des points \n",
        "  X_input = generate_latent_points (latent_dim,nb_images)\n",
        "  # prediction de la sortie du générateur\n",
        "  X = generator_model.predict(X_input)\n",
        "  # A ce niveau on considère que les images sont fausses\n",
        "  # donc on met 0 comme label. \n",
        "  y = zeros((nb_images, 1))\n",
        "  return X, y    \n",
        "\n",
        "def plot_and_save_generatedimages(generated_images, epoch, nb_images=10):\n",
        "\t# Affichage des images\n",
        "\tfor i in range(nb_images * nb_images):\n",
        "\t\tpyplot.subplot(nb_images, nb_images, 1 + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(generated_images[i, :, :, 0], cmap='gray_r')\n",
        "\t# sauvegarde de l'image\n",
        "\tfilename = 'generated_plot_FashionCNN_withGan%03d.png' % (epoch+1)\n",
        "\tpyplot.savefig(filename)\n",
        "\tpyplot.close()\n",
        " \n",
        "\n",
        "\n",
        "def evaluate_model (dataset, epoch, generator_model, discriminator_model, latent_dim, nb_images=100, save_model=True):\n",
        "  # récupération de vraies images pour le discriminateur\n",
        "  X_real, y_real = generate_real_samples(dataset, nb_images)\n",
        "  # Evaluation de l'accuracy pour le discriminateur\n",
        "  _, acc_real = discriminator_model.evaluate(X_real, y_real, verbose=0)\n",
        "\n",
        "  # génération de fausses images pour le générateur et donc le gan\n",
        "  X_fake, y_fake = generate_fake_samples_for_generator(generator_model, latent_dim, nb_images)\n",
        "  # Evaluation du discriminateur avec des fausses images\n",
        "  _, acc_fake = discriminator_model.evaluate(X_fake, y_fake, verbose=0)\n",
        "\n",
        "  print ('Accuracy reélle : %.0f%%, fausse : %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "  \n",
        "  plot_and_save_generatedimages(X_fake, epoch)\n",
        "  if save_model==True:\n",
        "    # sauvegarde du generateur pour un autre usage\n",
        "    filename = 'generator_model_FashionMNIST%03d.h5' % (epoch + 1)\n",
        "    generator_model.save(filename)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "uMpAj9agDePI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the generator and discriminator\n",
        "def train(generator_model, discriminator_model, gan_model, dataset, latent_dim, epochs=100, batchsize=256):\n",
        "  # Pour déterminer combien il y aura de batchs analysés à chaque epoch\n",
        "  batches_per_epoch = int(dataset.shape[0] / batchsize)\n",
        "\n",
        "  # pour récupérer le même nombre d'images fausses et vraies\n",
        "  half_batch = int(batchsize / 2)\n",
        "\n",
        "  for i in range(epochs):\n",
        "    # parcours des batches pour chaque pas d'epoch\n",
        "    for j in range(batches_per_epoch):\n",
        "\n",
        "      # PARTIE DISCRIMINATOR\n",
        "      #tirage aléatoire de half_batch images vraies\n",
        "      X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "      # generation de half_size fausses images\n",
        "      X_fake, y_fake = generate_fake_samples_for_generator(generator_model, latent_dim, half_batch)\n",
        "      # vstack permet de concaténer les vraies et fausses images dans X (resp. dans y)\n",
        "      # on aurait pu utiliser np.concatenate (np.concatenate([X_real, X_fake]))\n",
        "      X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
        "\n",
        "      # la ligne suivante permet au discriminateur de mettre à jour ses poids\n",
        "      # attention dans le GAN il ne peut pas aprendre donc il ne peut le faire qu'ici\n",
        "      discriminator_loss,_  = discriminator_model.train_on_batch(X, y)\n",
        "\n",
        "      # PARTIE GENERATOR ET DONC DE L'ENTREE DU GAN\n",
        "      # Création des points comme entrée du générateur \n",
        "      # \n",
        "      X_for_gan = generate_latent_points(latent_dim, batchsize)\n",
        "      # On met un 1 pour les labels des fausses images pour faire croire au discriminateur\n",
        "      # qu'il s'agit de vraies images\n",
        "      y_for_gan = ones((batchsize, 1))\n",
        "\n",
        "      # mise à jour des poids du générateur par propagation de l'esseur\n",
        "      # du discriminateur \n",
        "      gan_loss = gan_model.train_on_batch(X_for_gan, y_for_gan)\n",
        "\t\t\t# Affichage des loss\n",
        "      print('epoch %d - batch %d/%d, discriminator_loss=%.3f, generator_loss=%.3f'%(i+1, j+1, batches_per_epoch, discriminator_loss, gan_loss))\n",
        "\n",
        "    # Toutes les 20 epochs evaluation du modèle et sauvegarde du générateur\n",
        "    if i==1 or i % 20==0:\n",
        "        evaluate_model (dataset, i, generator_model, discriminator_model, latent_dim)\n",
        "      "
      ],
      "metadata": {
        "id": "XssqYHYas83D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "epochs=100\n",
        "batchsize=257\n",
        "# Creation du discriminateur\n",
        "discrimator_model = create_discriminator()\n",
        "# Creation du generateur\n",
        "generator_model = create_generator(latent_dim)\n",
        "# Creation du GAN\n",
        "gan_model = create_gan(generator_model, discriminator_model)\n",
        "# Chargement des données\n",
        "dataset = load_data()\n"
      ],
      "metadata": {
        "id": "4DPeZBLpat6q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il ne reste plus qu'à entraîner le modèle.   \n",
        "\n",
        "**CO2 !!! Remarque IMPORTANTE** Attention l'apprentissage est très long. Si vous voulez le faire il faudra décommenter la cellule suivante pour entraîner le modèle.   \n",
        "Pour éviter de consommer inutilement du CO2, le modèle a été appris (utilisation du GPU sous Colab) et sauvegardé. Il est disponible et utilisé dans les cellules d'après. "
      ],
      "metadata": {
        "id": "JQhJVUZeTaHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "#train(generator_model, discriminator_model, gan_model, dataset, latent_dim, epochs,batchsize)"
      ],
      "metadata": {
        "id": "XVgZrQGQTbBH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme annoncé précédemment, il est inutile de lancer le code précédent (Merci pour le CO2). Vous pouvez récupérer un modèle appris (utilisation du GPU sur Colab - aller dans le menu Modifier/Paramètres du notebook et sélectionner GPU). Vous pouvez télécharger le modèle : "
      ],
      "metadata": {
        "id": "oraLqtB14xHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.lirmm.fr/~poncelet/Ressources/generator_model_CNNFashion_100epochs.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tD1teYHq5PWb",
        "outputId": "7b571e15-5a2d-46f3-c9c2-3eb8c0a14580"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-29 20:46:02--  https://www.lirmm.fr/~poncelet/Ressources/generator_model_CNNFashion_100epochs.h5\n",
            "Resolving www.lirmm.fr (www.lirmm.fr)... 193.49.104.251\n",
            "Connecting to www.lirmm.fr (www.lirmm.fr)|193.49.104.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4680848 (4.5M) [text/plain]\n",
            "Saving to: ‘generator_model_CNNFashion_100epochs.h5.4’\n",
            "\n",
            "generator_model_CNN 100%[===================>]   4.46M  6.28MB/s    in 0.7s    \n",
            "\n",
            "2022-09-29 20:46:04 (6.28 MB/s) - ‘generator_model_CNNFashion_100epochs.h5.4’ saved [4680848/4680848]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici un exemple de données générées avec le modèle qui a été appris au cours des epochs :     \n",
        "\n",
        "EPOCH 1\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/generated_plot_e001.png\" align=\"center\" >\n",
        "\n",
        "EPOCH 30 \n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/generated_plot_e030.png\" align=\"center\" >\n",
        "\n",
        "EPOCH 70\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/generated_plot_e070.png\" align=\"center\" >\n",
        "\n",
        "EPOCH 100\n",
        "<IMG SRC=\"http://www.lirmm.fr/~poncelet/Ressources/generated_plot_e100.png\" align=\"center\" >"
      ],
      "metadata": {
        "id": "1ElOB-xIFGpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il suffit alors de charger le modèle appris pour générer des images :    "
      ],
      "metadata": {
        "id": "QRxYMAcs7B4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('generator_model_CNNFashion_100epochs.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sQv4-wkU7Jkr",
        "outputId": "6c20d0dc-0826-41b0-b473-76d14f8bacfa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour voir comment une image est générée il suffit de générer un vecteur de latent_dim et de le passer dans le modèle pour voir l'image finale associée. "
      ],
      "metadata": {
        "id": "T1261GxH7vbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'un vecteur aléatoire de taille latent_dim\n",
        "latent_points = generate_latent_points(100, 1)"
      ],
      "metadata": {
        "id": "hYSbs58S7--a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il suffit de prédire l'application du modèle :     "
      ],
      "metadata": {
        "id": "S81vsnHN8aX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image = model.predict(latent_points)"
      ],
      "metadata": {
        "id": "VEq4iFDR8Wxr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Affichage de l'image prédite, i.e. de l'image générée : "
      ],
      "metadata": {
        "id": "85s7lqhZ8mTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow (generated_image[0, :, :, 0], cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "93fxCTQe8pLD",
        "outputId": "d6e3d26b-2610-447e-c6e3-2707a86f19f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6bd4792e10>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQcElEQVR4nO3db4yV5ZnH8d8lgiIg/+VfyVIa/NOYLF0J0Wg2bpo2Lm+wL9QSs2GN7vRFNdSQuAZN6hsSs9mKS6IN06ilm64NCTaSqBGWNHGJCTIiyh+3yCKkIDAVogVBxmGufTGPzVTnue7pec45z4H7+0nIzJxr7nNuzvDjOXOu535uc3cBuPRdVvcEALQHYQcyQdiBTBB2IBOEHcjE5e18MDPjrX+gxdzdhru90pHdzO4ws9+b2QEze7TKfQFoLWu0z25moyTtl/Q9SUck7ZC0zN33BWM4sgMt1ooj+2JJB9z9oLv3SfqNpKUV7g9AC1UJ+xxJfxjy9ZHitr9gZl1m1mNmPRUeC0BFLX+Dzt27JXVLvIwH6lTlyH5U0twhX3+juA1AB6oS9h2SFpjZN81sjKQfStrUnGkBaLaGX8a7e7+ZPSjpdUmjJD3v7nubNjMATdVw662hB+N3dqDlWnJSDYCLB2EHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IRMNbNgMj8fDDD5fWbrnllnDswoULw/pLL70U1t95553S2ptvvhmOPXPmTFi/cOFCWE+ZPn16ae3WW28Nx7777rultf3795fWKoXdzA5JOi3pgqR+d19U5f4AtE4zjuz/4O4fN+F+ALQQv7MDmagadpe02czeNrOu4b7BzLrMrMfMeio+FoAKqr6Mv83dj5rZNZK2mNn/uvsbQ7/B3bsldUuSmXnFxwPQoEpHdnc/WnzslfRbSYubMSkAzddw2M1snJlN+PJzSd+XtKdZEwPQXObe2CtrM5uvwaO5NPjrwH+5++rEGF7GX2KefvrpsL5ixYrS2sDAQDj2ssviY1Hq366ZhfUqUnPv6+sL66NGjSqtnT9/Phzb1TXs22OSpNdff10nT54c9i/e8O/s7n5Q0t82Oh5Ae9F6AzJB2IFMEHYgE4QdyARhBzLRcOutoQej9XbJSf37iZaCppaJVm2dVRlf9bFTf7foeUuNnTRpUmmtv79fAwMDw06eIzuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5ngUtIIXX/99WE9tdTz9OnTpbUxY8aEY6NloCN57Cq98qrnn6SW50ZzP3v2bDj2iy++aGxODY0CcNEh7EAmCDuQCcIOZIKwA5kg7EAmCDuQCfrsCO3duzesp/rJqV56JNXrrnKp6VQPv2qfvb+/P6xffnl59KpuB12GIzuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5mgz45Qql+cEq0pT603b3W9ilQfPuqjj2R8KySP7Gb2vJn1mtmeIbdNMbMtZvZB8XFya6cJoKqRvIz/paQ7vnLbo5K2uvsCSVuLrwF0sGTY3f0NSae+cvNSSeuLz9dLurPJ8wLQZI3+zj7D3Y8Vnx+XNKPsG82sS1JXg48DoEkqv0Hn7h5t2Oju3ZK6JTZ2BOrUaOvthJnNkqTiY2/zpgSgFRoN+yZJy4vPl0t6uTnTAdAqyZfxZvaipNslTTOzI5J+KulJSRvM7H5JhyXd3cpJonVeeOGFsJ5aj97X1xfWozXnqT54lfXqKa3swUvV5pa6Hn6jkmF392Ulpe82eS4AWojTZYFMEHYgE4QdyARhBzJB2IFMWDuX2nEGXftNmDAhrH/66adhvcqWzFLcuku11qpe7rnO1lxqfLTtcupS0ldffXVYd/dhH5wjO5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmaDPfolL9cmjfq/U2l52qo9edaln1Mev+u8+1UdPzT1aGnzVVVeFY8eOHVta6+/v18DAAH12IGeEHcgEYQcyQdiBTBB2IBOEHcgEYQcywZbNl4BDhw6V1lJro1Pr2VO98NSlpqOti1O96FQvO/V3i1Tts1ede7QVdmqd/0033VRa2717d/n9hvcK4JJB2IFMEHYgE4QdyARhBzJB2IFMEHYgE/TZLwL33XdfWJ85c2ZprcqWylK1PrpUrZ9cdcvmqBdetc9edW5RPTV2+vTppbXo55E8spvZ82bWa2Z7htz2hJkdNbNdxZ8lqfsBUK+RvIz/paQ7hrl9jbsvLP682txpAWi2ZNjd/Q1Jp9owFwAtVOUNugfN7L3iZf7ksm8ysy4z6zGzngqPBaCiRsP+c0nfkrRQ0jFJPyv7RnfvdvdF7r6owccC0AQNhd3dT7j7BXcfkPQLSYubOy0AzdZQ2M1s1pAvfyBpT9n3AugMyT67mb0o6XZJ08zsiKSfSrrdzBZKckmHJP2ohXO85C1dujSsP/PMM2H9k08+Ka319vaGYydPLn27RVK6z57q40c949Sa8FQvu8r41Dr91PkDVUVr8VNr4W+88cbS2o4dO0pryb+Ruy8b5ubnUuMAdBZOlwUyQdiBTBB2IBOEHcgEYQcywRLXQqoVc8MNNzR834888khYX7IkXjQYLROVpM8//7y0Nn78+HDs6NGjw3qqvZVajhm1mFLPeeq+U625aDvqkydPhmNPnDgR1jdv3hzWZ8+eHdajlugDDzwQjn3ttddKa9GlwTmyA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQiY7qs998881h/dlnny2tzZo1q7QmSR9//HFYnzRpUliPetlTp04Nx6aWLJ4+fTqsp3rC0f1feeWV4dhW9tFHUo9UvVxzNH7KlCnh2NTS3/nz54f11BLZ6ByA1L/FaEl0d3d3aY0jO5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmWh7nz1aw7x69epwbNTPPnv2bDg21U9O9bqjvutnn30Wjj137lyl+vnz58N6dI5Bai186hyA1PhUrzs1vpWPfcUVVzQ8tsp9S+m5R+cARD14SZozZ05pLbr0N0d2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcy0dY++4IFC7R27drS+rx588Lx0Zr0a665Jhw7bty4sJ7qlUf11Hr2adOmhfWdO3eG9aeeeiqsr1u3rrSWOn8gdX5CSmq9etRvTm0HnZJaM16lx59aS5869yF1Tfyol576mUQ/0+jnkTyym9lcM/udme0zs71mtqK4fYqZbTGzD4qP8Wp/ALUaycv4fkkr3f3bkm6W9GMz+7akRyVtdfcFkrYWXwPoUMmwu/sxd99ZfH5a0vuS5khaKml98W3rJd3ZqkkCqO6veoPOzOZJ+o6k7ZJmuPuxonRc0oySMV1m1mNmPdE+VABaa8RhN7PxkjZK+om7/2lozQdXDQy7csDdu919kbsvmjhxYqXJAmjciMJuZqM1GPRfu/tLxc0nzGxWUZ8lqXxbSgC1S7bebHAN5HOS3nf3oT2gTZKWS3qy+Phy6r76+/t16tSp0nq0dE+KL/+baoVEl4KWpNSrjqi1V+WywZL01ltvhfXt27eH9ajNk3peql4Kuko99TNJLb9N1aPHTv3MUktcU1rZkpw+fXppLfp7jaTPfqukf5K028x2Fbet0mDIN5jZ/ZIOS7p7BPcFoCbJsLv7Nkll/4V+t7nTAdAqnC4LZIKwA5kg7EAmCDuQCcIOZKKtS1w//PBD3XvvvaX1hx56KBy/fPny0to999wTjk0tWUxt0fvRRx+V1vbt2xeO3bBhQ1jfvHlzWE8tl4x6q6ledOoS26leeEp0jkFq2fHo0aPDeup5ican7jvVJ6/6vET6+vrCenTaeaUlrgAuDYQdyARhBzJB2IFMEHYgE4QdyARhBzLR9i2bI9Fad0las2ZNQ7WLXaofHa2NHjt2bDg21U9O9eFTWxdH/eiTJ0+GY1P/HmbPnh3Wo/MPUn321GWoUz+TKudGpNbaR895dD4JR3YgE4QdyARhBzJB2IFMEHYgE4QdyARhBzLRUX12DC+1nfTq1atLaytXrgzHpq5pn+o3p7b0evXVV0tr27ZtC8f29sb7jjz++ONh/dprry2tHT58OBybOr/guuuuC+up685PmDChtBZtTS5JBw4cKK1F5zVwZAcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBMj2Z99rqRfSZohySV1u/t/mNkTkv5F0h+Lb13l7uVNVbTM2rVrS2t33XVXOHbv3r1h/eDBg2F9//79Yf2VV14praWuaT9z5sywnrq+erTP+cSJE8OxqbX0qXMApk6dGtbPnTtXWquylj5aRz+Sk2r6Ja10951mNkHS22a2paitcfd/H8F9AKjZSPZnPybpWPH5aTN7X9KcVk8MQHP9Vb+zm9k8Sd+RtL246UEze8/MnjezYfdPMrMuM+sxs55KMwVQyYjDbmbjJW2U9BN3/5Okn0v6lqSFGjzy/2y4ce7e7e6L3H1RE+YLoEEjCruZjdZg0H/t7i9JkrufcPcL7j4g6ReSFrdumgCqSobdBt8yfU7S++7+1JDbZw35th9I2tP86QFoFkstxTOz2yT9j6Tdkr5c97dK0jINvoR3SYck/ah4My+6r/jB0JDo8sGpFlLqUtKrVq0K6+vWrQvrVaRac1W3o+5Uqe3Fo7/3hQsX5O7DfsNI3o3fJmm4wfTUgYsIZ9ABmSDsQCYIO5AJwg5kgrADmSDsQCa4lPQlIOonb9y4MRz72GOPhfXjx483NKdmSJ0DkqpfrFp1fgBHdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMpFcz97UBzP7o6She+VOkxTvT1ufTp1bp85LYm6Naubc/sbdpw9XaGvYv/bgZj2dem26Tp1bp85LYm6NatfceBkPZIKwA5moO+zdNT9+pFPn1qnzkphbo9oyt1p/ZwfQPnUf2QG0CWEHMlFL2M3sDjP7vZkdMLNH65hDGTM7ZGa7zWxX3fvTFXvo9ZrZniG3TTGzLWb2QfFx2D32aprbE2Z2tHjudpnZkprmNtfMfmdm+8xsr5mtKG6v9bkL5tWW563tv7Ob2ShJ+yV9T9IRSTskLXP3fW2dSAkzOyRpkbvXfgKGmf29pDOSfuXuNxa3/ZukU+7+ZPEf5WR3/9cOmdsTks7UvY13sVvRrKHbjEu6U9I/q8bnLpjX3WrD81bHkX2xpAPuftDd+yT9RtLSGubR8dz9DUlf3dJlqaT1xefrNfiPpe1K5tYR3P2Yu+8sPj8t6cttxmt97oJ5tUUdYZ8j6Q9Dvj6iztrv3SVtNrO3zayr7skMY8aQbbaOS5pR52SGkdzGu52+ss14xzx3jWx/XhVv0H3dbe7+d5L+UdKPi5erHckHfwfrpN7piLbxbpdhthn/szqfu0a3P6+qjrAflTR3yNffKG7rCO5+tPjYK+m36rytqE98uYNu8bG35vn8WSdt4z3cNuPqgOeuzu3P6wj7DkkLzOybZjZG0g8lbaphHl9jZuOKN05kZuMkfV+dtxX1JknLi8+XS3q5xrn8hU7Zxrtsm3HV/NzVvv25u7f9j6QlGnxH/v8kPVbHHErmNV/Su8WfvXXPTdKLGnxZ94UG39u4X9JUSVslfSDpvyVN6aC5/acGt/Z+T4PBmlXT3G7T4Ev09yTtKv4sqfu5C+bVlueN02WBTPAGHZAJwg5kgrADmSDsQCYIO5AJwg5kgrADmfh/cqHrwSwpMkIAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}